\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}

\begin{document}
\title{Machine Learning - Exercise 1 }
\author{Deniz Kocabas and Hans-JÃ¶rg Schurr}

\maketitle
\tableofcontents
\newpage

Our end report should contain:

\begin{itemize}
	\item Report should be around 10 pages
	\item Full report of your work
	\item Experiments, parameters tried
	\item Characteristics of data sets \& pre-processing (i.e.
handling of missing values, scaling etc.)
	\item Characteristics of regression techniques
    \item Explanation of choice for data sets \& techniques
	\item Discuss experimental results, compare them in regard of
the different datasets \& techniques (tables, figures)
\end{itemize}

\subsection{Introduction}
The Data Intensive Science has become an indispensable aspect of the people's lives. We use it in many different areas to gain knowledge by analysis of integrated data. The overall goal of that assignment is to perform experiments with regression techniques in machine learning. Regression is the easiest technique to use, but also the least powerful. There are a number of independent variables, which, when taken together, produce a result a dependent variable. The regression model is then used to predict the result of an unknown dependent variable, given the values of the independent variables. 

We had to pick three datasets from UCI Machine Learning Repository, which should have different characteristics. 
Our choice was;
\begin{enumerate}
    \item "Individual Household Electric Power Consumption Data Set" (http://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption)
    \item "Solar Flare Data Set"
        (http://archive.ics.uci.edu/ml/datasets/Solar+Flare)
    \item "Wine Quality Data Set"
        (http://archive.ics.uci.edu/ml/datasets/Wine+Quality)
\end{enumerate}

We have used 4 different regression techniques by the tool Weka. 
These techniques were;
\begin{enumerate}
    \item Simple Linear Regression
    \item Linear Regression
    \item SMOReg
    \item M5P
\end{enumerate}

\subsection{The chosen datasets}
The household electric power consumption data set contains over two million
instances and is by far the largest dataset among those three sets. The dataset
has nine dimensions, whereby two fields are used for a time stamp. Thus it is a
timeseries. All other fields contain numeric data. According to the
documentation 1.25 percent of the rows contain missing values. We decided to use
this dataset to test the performance of the methods. Given the high number of
missing values, this dataset is suited to test different methods to handle those
missing values. This dataset contains 3 class labels, which are sub-metering No. 1, 
energy sub-metering No. 2,  energy sub-metering No. 3 represents the active energy 
consumed every minute in watt hour. No. 1 corresponds to the kitchen, containing mainly 
a dishwasher, an oven and a microwave (hot plates are not electric but gas powered). 
No. 2 corresponds to the laundry room, containing a washing-machine, a tumble-drier, 
a refrigerator and a light. And No. 3 corresponds to an electric water-heater and an air-conditioner. 

The solar flare data set is with roughly 1400 instances much smaller then the
household electric power consumption data set. Every instance has ten fields,
which contain mainly categorical data. Some of those map to numeric intervals
(area of the spots). The set lacks missing values, therefore it is well suited
to test various methods to handle categorical data. The database has 3 class labels, 
one for the number times a certain type of solar flare occured in a 24 hour period. 
These 3 class labels are C-class flares (common flares), M-class flares (moderate flares), 
X-class flares (severe flares) production by this region in the following 24 hours. 
Each instance represents captured features for 1 active region on the sun. 
There are 2 files and the second file has had much more error correction applied to the it, 
and has consequently been treated as more reliable. 

We decided to choose a set with medium size and mixed features as third set. The
wine quality data set provided those requirements. It contains around 5000
instances and no missing values. The twelve attributes contain mainly numeric
values. Most of those are chemical properties of the wines. The last field is
the class value is based on sensory data, median of at least 3 evaluations made by wine experts. 
Each expert graded the wine quality  an assigned quality of the wine. It is natural number 
between one (very bad) and ten (very excellent).
Given the fact, that it is not exactly measured it may behave like a
categorical value. There are two dataset, one is for the red wine and the other for white wine.

\subsection{Methods tried}

    The Simple Linear Regression techniques is the Linear Regression with one variable. The algorithm in Weka picks the attribute with the lowest squared error. The algorithm can not deal with the missing values, so we need to preprocess before the algorithm and also just the numeric values should be in the attributes, otherwise weka can not use this algorithm. We should remove or preprocess these other types of dimensions.

     The Linear Regression techniques is for the multiple variable. The algorithm in Weka uses the Akaike criterion. It is a measure of the relative quality of a statistical model, for a given set of data. As such, AIC provides a means for model selection. The algorithm is able to deal with weighted instances.

     The algorithm SMOReg in weka is the support vector machine for regression. The parameters can be learned using various algorithms. The algorithm is selected by setting the RegOptimizer. This implementation globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes by default.

     M5 Prunning(M5P)  is a non-linear regression, which implements base routines for generating M5 Model trees and rules. A model tree is a tree, where each leaf has got one of these linear regression model. It is like a patch work of linear models.  

\subsection{Experiments and results for the individual datasets}
\subsubsection{The household electric power consumption data set}
\subsubsection{The solar flare data set}
The solar flare file contains 2 datase. We have chosen the smallest dataset, which has around 300 instances. (flare1.csv) This file has the smallest instance in our datasets. First we convert the file into excel file and upload it to weka. In this file, there are no missing values. So we do not need any preprocess method. In that dataset we have also some attributes are in nominativ type. And start the algorithms in the classify section of the weka. There are 3 class variable in that dataset, we calculate the regression model for the class X flares (severe flares) production by this region in the following 24 hours. 

We have used the 10 folds Cross-validation techniques to split the data for test and train. It divides the dataset into 10 part and each 10 samples is retained as the validation data for testing the model, and the remaining 9 samples are used as training data. The cross-validation process is then repeated 10 times, with each of the 10 samples used exactly once as the validation data. The 10 results from the folds then can be averaged to produce a single estimation.  

At first we have run the Simple Linear Regression techniques, which is the least powerful algorithm. This technique can just deal with the Numeric attributes. For this reason, we deleted the nominativ dimension from the dataset, which are the first 3 fields. It chooses the attribute Historically-complex, which hast the lowest squared error. Linear regression on Historically-complex model is; 

$0.05 * Historically-complex - 0.04$\\

After that we have used the Linear Regression techniques for the multiple variables. We added the nominativ attributes again to our dataset and run the algorihm. The technique choose some of the attributes for the regression model. The model is;\\

$   X-class flares =   0.0334 * Code for class=H,D +
      0.1165 * Code for spot distribution=C +
      0.0271 * Historically-complex +
     -0.038 $\\

So the quality depends on these variables. We can see above that two nominativ attribute called "Code for class" and "Code for spot distribution" are in our linear regression model.

The 3. technique is the SMOReg, the support vector machine for regression. In that technique all of the attributes have used in the model. We can see from the below that for the nominativ attribute all possible values are also in the model. Some of them has a negative marks, which can increase the class value, when we decrease these attributes. And for the positive weight, we need to increase.The model is;\\
$weights (not support vectors):
 -       0.0001 * (normalized) Code for class=C
 +       0.0002 * (normalized) Code for class=D
 +       0.0002 * (normalized) Code for class=B
 +       0      * (normalized) Code for class=F
 +       0.0002 * (normalized) Code for class=H
 -       0.0006 * (normalized) Code for class=E
 -       0.0002 * (normalized) Code for largest spot size=S
 +       0.0004 * (normalized) Code for largest spot size=A
 -       0.0001 * (normalized) Code for largest spot size=K
 +       0.0002 * (normalized) Code for largest spot size=R
 +       0.0002 * (normalized) Code for largest spot size=X
 -       0.0005 * (normalized) Code for largest spot size=H
 +       0.0002 * (normalized) Code for spot distribution=O
 -       0.0007 * (normalized) Code for spot distribution=I
 +       0.0002 * (normalized) Code for spot distribution=X
 +       0.0002 * (normalized) Code for spot distribution=C
 -       0.0006 * (normalized) Activity
 +       0.0003 * (normalized) Evolution
 +       0.0003 * (normalized) Previous 24 hour flare activity code
 +       0.001  * (normalized) Historically-complex
 +       0      * (normalized) Did region become historically complex on this pass across the sun's disk
 +       0.0002 * (normalized) Area
 +       0.0004 * (normalized) Area of the largest spot
 -       0.0007$\\

And the last one is the tree model, M5 Prunning(M5P). In that model we have many leaf of the trees, which has a different linear regression model. In solar flare file, the algorithm generate 2 leaf nodes.

\begin{figure}[ht!]
\centering
\includegraphics[width=120mm]{tree}
\caption{A simple caption}
\label{overflow}
\end{figure}

That means, there are 2 different linear regression model, which have shown in the below.\\
LM num: 1: \\
$X-class flares = 
	-0.0067 * Code for class=H,D 
	+ 0.0093 * Code for class=D 
	+ 0.0095 * Code for spot distribution=X,C 
	+ 0.0022 * Historically-complex 
	- 0.0031$\\
LM num: 2:\\
$X-class flares= 
	-0.0074 * Code for class=H,D 
	+ 0.1174 * Code for class=D 
	+ 0.1292 * Code for spot distribution=X,C 
	+ 0.0552 * Historically-complex 
	- 0.1607$

\subsubsection{The wine quality data set}

The wine quality dataset contains 2 file, one is for the white wine and the other is red wine. We have chosen the white wine dataset, which has almost 5000 instances. First we convert the file to csv and upload it to weka. In this file, there are no missing values and all the attributes are real numbers. So we do not need any preprocess method. And start the algorithms in the clasify section of the weka. The class variable is quality attribute and its the number from one to ten. We have also used the 10 folds Cross-validation techniques to split the data for test and train. 

At first we have run the Simple Linear Regression techniques, which is the least powerful algorithm. It chooses the attribute alcohol, which hast the lowest squared error. Linear regression on alcohol model is; 

$0.31 * alcohol + 2.58$\\

After that we have used the Linear Regression techniques for the multiple variables. The technique choose some of the attributes for the regression model. The model is;\\
$quality =
      0.0681 * fixed acidity +
     -1.8881 * volatile acidity +
      0.0828 * residual sugar +
      0.0033 * free sulfur dioxide +
   -154.2913 * density +
      0.6942 * pH +
      0.6285 * sulphates +
      0.1932 * alcohol +
    154.1062$\\

So the quality depends on these variables above. Some of them has a negative marks, that means, if we decrease these attributes, we have better quality of wine. And for the positive weight, we need to increase.

The 3. technique is the SMOReg, the support vector machine for regression. In that technique all of the attributes have used in the linear regression model. The model is;\\
$weights (not support vectors):
 +       0.0754 * (normalized) fixed acidity
 -       0.3535 * (normalized) volatile acidity
 -       0.0291 * (normalized) citric acid
 +       0.7259 * (normalized) residual sugar
 -       0.0609 * (normalized) chlorides
 +       0.2402 * (normalized) free sulfur dioxide
 -       0.049  * (normalized) total sulfur dioxide
 -       0.9238 * (normalized) density
 +       0.1421 * (normalized) pH
 +       0.1072 * (normalized) sulphates
 +       0.2417 * (normalized) alcohol
 +       0.3849$\\

And the last one is the tree model, M5 Prunning(M5P). In that model we have many leaf of the trees, which has a different linear regression model. In the wine quality dataset, the algorithm generate 55 leaf, different linear regression. It is quiet hard to show all of them and also the tree is a bit complicated in that dataset.


\subsubsection{Comparison}

\begin{table}
\begin{tabular}{|l| c | c |c |c |c |}

\hline & & & & & \\
Data & Tech. & Time(s) & Correlation & Mean abs.  & Root mean \\
 & & & coefficient & error & squared error  \\
\hline & & & & &  \\
1  & 1  &   &   &  &  \\ 
\hline & & & & & \\
1  & 2  &   &   &  &  \\ 
\hline & & & & & \\
1  & 3  &   &   &  &  \\ 
\hline & & & & & \\
1  & 4  &   &   &  &  \\ 
\hline & & & & & \\
2  & 1  & 0  & 0.0574  & 0.0425 & 0.146 \\ 
\hline & & & & & \\
2  & 2  & 0.02  &  0.1028 & 0.0471 & 0.1469  \\ 
\hline & & & & & \\
2  & 3  & 0.88  & -0.0092 & 0.0223 & 0.1472 \\ 
\hline & & & & & \\
2  & 4  & 0.11  & 0.0625  & 0.046   & 0.1497 \\ 
\hline & & & & & \\
3  & 1  & 0.26   & 0.4345  & 0.628   &  0.7976\\ 
\hline & & & & & \\
3  & 2  & 0.08  & 0.5257  & 0.5854 & 0.7534  \\ 
\hline & & & & & \\
3  & 3  & 94.03  &  0.5225  & 0.5841 & 0.7564 \\ 
\hline & & & & & \\
3  & 4  & 1.08  & 0.5866  & 0.5525  & 0.7188 \\ 
\hline
\end{tabular}
\caption{\textbf{Results of the Regression Techniques}
    Algorithm 1: Simple Linear Regression
    Algorithm 2: Linear Regression
    Algorithm 3: SMOReg
    Algorithm 4: M5P
    Dataset 1: "Individual Household Electric Power Consumption Data Set" 
    Dataset 2: "Solar Flare Data Set"
    Dataset 3: "Wine Quality Data Set"}
\end{table}


\subsection{Conclusion}

\end{document}
